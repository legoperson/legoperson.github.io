<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</title><meta name="author" content="Haoyang Wen ; Ying Lin ; Tuan Lai ; Xiaoman Pan ; Sha Li ; Xudong Lin ; Ben Zhou ; Manling Li ; Haoyu Wang ; Hongming Zhang ; Xiaodong Yu ; Alexander Dong ; Zhenhailong Wang ; Yi Fung ; Piyush Mishra ; Qing Lyu ; Dídac Surís ; Brian Chen ; Susan Windisch Brown ; Martha Palmer ; Chris Callison-Burch ; Carl Vondrick ; Jiawei Han ; Dan Roth ; Shih-Fu Chang ; Heng Ji"/><meta name="description" content="naacl 2021"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s4 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; margin:0pt; }
 .s7 { color: #00007F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s8 { color: #00007F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s10 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s11 { color: #00007F; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s13 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .h3, h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s14 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s15 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt; }
 .s16 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s17 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 1pt; }
 .s18 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt; vertical-align: 5pt; }
 .s19 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s20 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s21 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt; vertical-align: -3pt; }
 .s22 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 3pt; }
 .s23 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s24 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s25 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s26 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -3pt; }
 .s28 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s29 { color: black; font-family:Arial-BoldItalicMT, sans-serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s30 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s31 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s32 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s35 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 7pt; vertical-align: 3pt; }
 .s36 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s37 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s38 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s39 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: 1pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; }
 .s41 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s42 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s43 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s44 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s45 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -5pt; }
 .s47 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .s48 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; }
 .s49 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s50 { color: #808080; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s51 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8.5pt; }
 .s52 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s53 { color: #808080; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt; }
 .s54 { color: #7DA6DF; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s55 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s56 { color: #4D4D4D; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s57 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: 1pt; }
 .s58 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .a { color: #00007F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s59 { color: #00007F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l3 {padding-left: 0pt;counter-reset: c2 1; }
 #l3> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><h1 style="padding-top: 4pt;padding-left: 31pt;text-indent: 0pt;text-align: center;">RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</h1><h2 style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">Haoyang Wen<span class="s1">1</span>, Ying Lin<span class="s1">1</span>, Tuan Manh Lai<span class="s1">1</span>, Xiaoman Pan<span class="s1">1</span>, Sha Li<span class="s1">1</span>, Xudong Lin<span class="s1">2</span>, Ben Zhou<span class="s1">3 </span>Manling Li<span class="s1">1</span>, Haoyu Wang<span class="s1">3</span>, Hongming Zhang<span class="s1">3</span>, Xiaodong Yu<span class="s1">3</span>, Alexander Dong<span class="s1">3</span></h2><h2 style="padding-left: 44pt;text-indent: 0pt;text-align: center;">Zhenhailong Wang<span class="s1">1</span>, Yi Ren Fung<span class="s1">1</span>, Piyush Mishra<span class="s1">4</span>, Qing Lyu<span class="s1">3</span>, Dídac Surís<span class="s1">2 </span>Brian Chen<span class="s1">2</span>, Susan Windisch Brown<span class="s1">4</span>, Martha Palmer<span class="s1">4</span>, Chris Callison-Burch<span class="s1">3 </span>Carl Vondrick<span class="s1">2</span>, Jiawei Han<span class="s1">1</span>, Dan Roth<span class="s1">3</span>, Shih-Fu Chang<span class="s1">2</span>, Heng Ji<span class="s1">1</span></h2><p class="s2" style="padding-left: 31pt;text-indent: 0pt;text-align: center;">1<span class="s3"> University of Illinois at Urbana-Champaign </span>2<span class="s3"> Columbia University</span></p><p class="s2" style="padding-left: 31pt;text-indent: 0pt;text-align: center;">3<span class="s3"> University of Pennsylvania </span>4<span class="s3"> University of Colorado, Boulder</span></p><p class="s4" style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: center;">hengji@illinois.edu,sc250@columbia.edu,danroth@seas.upenn.edu</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: center;">Abstract</h2><p class="s5" style="padding-top: 7pt;padding-left: 34pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark0" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">We present a new information extraction sys- tem that can automatically construct temporal event graphs from a collection of news doc- uments from multiple sources, multiple lan- guages (English and Spanish for our experi- ment), and multiple data modalities (speech, text, image and video).  The system ad- vances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human cu- rated event schema library to match and en- hance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub</a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt;">1</span><a href="#bookmark1" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">, with a demo video</a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt;">2</span>.</p><ol id="l1"><li data-list-text="1"><h2 style="padding-top: 7pt;padding-left: 35pt;text-indent: -17pt;text-align: left;">Introduction</h2><p class="s8" style="padding-top: 7pt;padding-bottom: 4pt;padding-left: 18pt;text-indent: 0pt;line-height: 107%;text-align: right;"><a href="#bookmark35" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (</a><a href="#bookmark35" class="s7">Glavaš and </a>Štajner<a href="#bookmark35" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2013<a href="#bookmark34" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark34" class="s7">Glavaš et </a>al.<a href="#bookmark34" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2014<a href="#bookmark24" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark24" class="s7">Choubey et </a>al.<a href="#bookmark24" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark66" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">), aiding natural disaster relief efforts (</a>Panem et al.<a href="#bookmark66" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2014<a href="#bookmark81" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark81" class="s7">Zhang et </a>al.<a href="#bookmark81" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018<a href="#bookmark58" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark58" class="s7">Medina Maza et </a>al.<a href="#bookmark58" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark28" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">), fi- nancial analysis (</a><a href="#bookmark28" class="s7">Ding et </a>al.<a href="#bookmark28" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2014<a href="#bookmark29" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark78" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark78" class="s7">Yang et </a>al.<a href="#bookmark78" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018<a href="#bookmark40" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark40" class="s7">Jacobs et </a>al.<a href="#bookmark40" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018<a href="#bookmark33" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark33" class="s7">Ein-Dor et </a>al.<a href="#bookmark33" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<a href="#bookmark64" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark64" class="s7">Özbayoglu et </a>al.<a href="#bookmark64" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark69" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) and healthcare mon- itoring (</a><a href="#bookmark69" class="s7">Raghavan et </a>al.<a href="#bookmark69" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2012<a href="#bookmark41" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark41" class="s7">Jagannatha and </a>Yu<a href="#bookmark41" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark43" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark43" class="s7">Klassen et </a>al.<a href="#bookmark43" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark42" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark42" class="s7">Jeblee and </a>Hirst<a href="#bookmark42" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018<span style=" color: #000;">). However, it’s much more difficult to remem- ber event-related information compared to entity- related information. For example, most people in</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="80" height="1" alt="image" src="wen_files/Image_001.png"/></span></p><p class="s9" style="padding-top: 1pt;padding-left: 17pt;text-indent: 12pt;text-align: left;"><a name="bookmark0">1</a><a href="https://github.com/RESIN-KAIROS/RESIN-pipeline-public" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">Github: </a><a href="https://github.com/RESIN-KAIROS/RESIN-pipeline-public" class="s11" target="_blank">https://github.com/RESIN-KAIROS/RESI N-pipeline-public</a><a name="bookmark1">&zwnj;</a></p><p class="s9" style="padding-left: 17pt;text-indent: 12pt;text-align: left;">2<a href="http://blender.cs.illinois.edu/software/resin/resin.mp4" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;" target="_blank">Video: </a><a href="http://blender.cs.illinois.edu/software/resin/resin.mp4" class="s11" target="_blank">http://blender.cs.illinois.edu/softwa re/resin/resin.mp4</a></p><p style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;line-height: 107%;text-align: right;">the United States will be able to answer the ques- tion “Which city is Columbia University located in?”, but very few people can give a complete an- swer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, <i>first-generation</i>, au- tomated event understanding is overly simplistic since most methods focus on sentence-level se- quence labeling for event extraction. Existing meth- ods for complex event understanding also lack of incorporating knowledge in the form of a repository of abstracted event schemas (complex event tem- plates), understanding the progress of time via tem- poral event tracking, using background knowledge, and performing global inference and enhancement. To address these limitations, in this paper we will demonstrate a new end-to-end open-source dock- erized research system to extract temporally or- dered events from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). Our system consists of a pipeline of components that involve schema-guided entity, relation and complex event extraction, entity and event coreference res- olution, temporal event tracking and cross-media entity and event grounding. Event schemas encode knowledge of stereotypical structures of events and their connections. Our end-to-end system has been dockerized and made publicly available for</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">research purpose.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2"><h2 style="padding-left: 31pt;text-indent: -17pt;text-align: left;">Approach</h2><ol id="l2"><li data-list-text="2.1"><h3 style="padding-top: 10pt;padding-left: 37pt;text-indent: -24pt;text-align: left;">Overview</h3><p class="s8" style="padding-top: 7pt;padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark2" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The architecture of our system is illustrated in Fig- ure </a>1<span style=" color: #000;">. Our system extracts information from multi- lingual multimedia document clusters. Each docu-</span></p><p style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: center;">133</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-left: 126pt;text-indent: 0pt;line-height: 82%;text-align: center;">Proceedings of NAACL-HLT 2021: Demonstrations<span class="s10">, pages 133–143 June 6–11, 2021. ©2021 Association for Computational Linguistics</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="602" height="297" alt="image" src="wen_files/Image_002.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="40" height="32" alt="image" src="wen_files/Image_003.png"/></span></p><p class="s15" style="padding-left: 31pt;text-indent: -31pt;text-align: left;">Visual Event and Argument Extraction</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: left;">Multi-Modal Event Coreference</p><p style="text-indent: 0pt;text-align: left;"/><p class="s16" style="text-indent: 0pt;line-height: 15pt;text-align: left;">n<span class="s17">p</span>D%<span class="s17">%</span>12<span class="s17">3</span>%2<span class="s17">B</span>3%<span class="s17">h</span>B2<span class="s17">t</span>w0<span class="s17">m</span><span class="s18">M</span><span class="s17">l</span><span class="s18">u</span><span class="s17">%</span><span class="s18">lt</span><span class="s17">3</span><span class="s18">im</span><span class="s17">D</span>Se<span class="s17">1</span><span class="s18">ed</span><span class="s17">%</span>a<span class="s18">i</span><span class="s17">3</span><span class="s18">a</span><span class="s17">B</span>e%<span class="s17">f</span>%2<span class="s17">i</span>32<span class="s17">l</span>Dr<span class="s17">l</span>wo<span class="s17">C</span>ru<span class="s17">o</span>an<span class="s17">l</span>pd<span class="s17">o</span>%e<span class="s17">r</span>3d<span class="s17">%</span>B%<span class="s17">3</span>h3<span class="s17">D</span>tD<span class="s17">%</span>m1<span class="s17">2</span>l%<span class="s17">3</span>%3<span class="s17">f</span>3B<span class="s17">f</span><span class="s19">C</span>Dw<span class="s17">f</span>1h<span class="s19">a</span><span class="s17">2</span>%<span class="s19">s</span>i<span class="s17">c</span><span class="s19">s</span>3t<span class="s17">c</span><span class="s19">i</span>B<span class="s19">f</span>e<span class="s19">i</span><span class="s17">%</span><span class="s19">c</span>fS<span class="s17">3</span><span class="s19">a</span>ip<span class="s19">t</span><span class="s17">B</span><span class="s19">i</span>l<span class="s19">o</span>a<span class="s17">s</span>l<span class="s19">n</span>c<span class="s17">t</span>Ce<span class="s17">r</span>o%<span class="s17">o</span>l3<span class="s17">k</span>oD<span class="s17">e</span>rw<span class="s17">C</span>%r<span class="s17">o</span>3a<span class="s17">l</span>Dp<span class="s17">o</span>%%<span class="s17">r</span>23<span class="s17">%</span>3B<span class="s17">3</span>fh<span class="s17">D</span>ft<span class="s17">%</span>fm<span class="s17">2</span>2l<span class="s17">3</span>c%<span class="s17">d</span>c3<span class="s17">6</span>%D<span class="s17">b</span>3<span class="s20">G</span>1<span class="s17">6</span>B%<span class="s20">r</span><span class="s17">5</span><span class="s20">o</span>s3<span class="s17">6</span><span class="s20">u</span>tB<span class="s20">n</span><span class="s17">%</span>rf<span class="s20">d</span><span class="s17">3</span>o<span class="s20">i</span>i<span class="s17">B</span><span class="s20">n</span>kl<span class="s20">g</span><span class="s17">%</span>el<span class="s17">2</span>CC<span class="s17">2</span>oo<span class="s17">%</span>ll<span class="s17">2</span>oo<span class="s17">0</span>rr<span class="s17">v</span>%%<span class="s17">e</span>33<span class="s17">r</span>DD<span class="s17">t</span>%%<span class="s17">e</span>22<span class="s17">x</span>33<span class="s17">%</span>df<span class="s17">3</span>6f<span class="s17">D</span>bf<span class="s17">%</span>62<span class="s17">2</span>5c<span class="s17">2</span>6c<span class="s17">1</span>%%<span class="s17">%</span>33<span class="s17">2</span>BB<span class="s17">2</span>%s<span class="s17">%</span>2t<span class="s17">2</span>2r<span class="s17">0</span>%</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">Document clusters</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">M<span class="s19">D</span>u<span class="s19">o</span>lt<span class="s19">c</span>il<span class="s19">u</span>in<span class="s19">m</span>g<span class="s19">e</span>u<span class="s19">n</span>a<span class="s19">t </span>l<span class="s19">clusters</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">ResNet-50-based Event</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">Weakly-Supervised Event</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: left;">Docu<span class="s22">hs</span>m<span class="s22">it</span>e<span class="s22">ty</span>n<span class="s22">el</span>t C<span class="s22">p%</span>lu<span class="s22">3c</span>s<span class="s22">D</span>ters</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 5pt;text-align: left;">Schema Repository</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: center;">Automatic Speech Recognition</p><p class="s19" style="padding-top: 4pt;text-indent: 0pt;text-align: center;">Amazon Transcript API</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 17pt;text-indent: -17pt;text-align: left;">Background KB</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 5pt;text-indent: -5pt;text-align: left;">Entity, Relation, Event Mention Extraction</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 7pt;text-indent: -7pt;text-align: left;">Cross-Document Cross- Lingual Coreference</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-left: 8pt;text-indent: 2pt;text-align: left;">Temporal Event Graph in English</p><p class="s23" style="padding-top: 4pt;padding-left: 47pt;text-indent: 0pt;text-align: left;">Textual pipeline</p><p class="s15" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">Temporal Ordering</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: left;">Instantiated</p><p style="text-indent: 0pt;text-align: left;"/><p class="s16" style="text-indent: 0pt;text-align: left;">3Bhtml%3D<span class="s24">E</span>1<span class="s24">x</span>%<span class="s24">t</span>3<span class="s24">ra</span>B<span class="s24">c</span>f<span class="s24">ti</span>i<span class="s24">on</span>llColor%3D%23ff<span class="s24">a</span>f<span class="s24">n</span>2<span class="s24">d</span>c<span class="s24">N</span>c<span class="s24">I</span>%<span class="s24">L</span>3<span class="s24">C</span>B<span class="s24">lu</span>s<span class="s24">s</span>t<span class="s24">t</span>r<span class="s24">er</span>o<span class="s24">in</span>k<span class="s24">g</span>eColor%3D%<span class="s24">M</span>2<span class="s24">a</span>3<span class="s24">c</span>d<span class="s24">hi</span>6<span class="s24">n</span>b<span class="s24">e</span>656%3<span class="s25">R</span>B<span class="s25">e</span>%<span class="s25">la</span>2<span class="s25">ti</span>2<span class="s25">on</span>%2<span class="s25">E</span>0<span class="s25">x</span>v<span class="s25">tr</span>e<span class="s25">ac</span>r<span class="s25">t</span>t<span class="s25">io</span>e<span class="s25">n</span>x%3D%221%22%20pa</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">BERT-based Joint Mention</p><p style="text-indent: 0pt;text-align: left;"/><p class="s26" style="text-indent: 0pt;text-align: left;">pl<span class="s19">C</span>a<span class="s19">o</span>c<span class="s19">l</span>e<span class="s19">le</span>h<span class="s19">c</span>o<span class="s19">t</span>l<span class="s19">i</span>d<span class="s19">ve Ent</span>p<span class="s19">it</span>l<span class="s19">y</span>ac<span class="s19">L</span>e<span class="s19">in</span>h<span class="s19">k</span>o<span class="s19">i</span>l<span class="s19">n</span>d<span class="s19">g</span>er</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">x</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">T5-based Temporal</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;text-align: left;">Schema</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 3pt;text-align: left;">BART-based Document- Level Argument Extraction</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">Entity</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">Event</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-left: 8pt;text-indent: -8pt;line-height: 111%;text-align: left;">Translation x</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: left;">Coreference Coreference</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="text-indent: 0pt;text-align: center;">RoBERTa-based Temporal Relation Extraction</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: center;">Schema Matching</p><p class="s19" style="padding-top: 7pt;padding-left: 1pt;text-indent: 0pt;text-align: center;">LCS-based Beam Search</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Rule-based Event Coreference</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Joint Situation Localizer</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 4pt;padding-left: 21pt;text-indent: 0pt;text-align: left;"><a name="bookmark2">Figure 1: The architecture of RESIN schema-guided information extraction and temporal event tracking system.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark3" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ment cluster contains documents about a specific complex event. Our textual pipeline takes input from texts and transcribed speeches. It first extracts entity, relation and event mentions (Section </a>2.2<a href="#bookmark6" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">-</a>2.3<a href="#bookmark7" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) and then perform cross-document cross-lingual en- tity and event coreference resolution (Section </a>2.4<a href="#bookmark9" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). The extracted events are then ordered by tempo- ral relation extraction (Section </a>2.5<a href="#bookmark10" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). Our visual pipeline takes images and videos as input and ex- tracts events and arguments from visual signals and ground the extracted knowledge elements onto our extracted graph via cross-media event coreference resolution (Section </a>2.6<a href="#bookmark12" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). Finally, our system se- lects the schema from a schema repository that best matches the extracted IE graph and merges these two graphs (Section </a>2.7<a href="#bookmark4" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). Our system can extract 24 types of entities, 46 types of relations and 67 types of events as defined in the DARPA KAIROS</a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">3</span> <span style=" color: #000;">ontology.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.2"><h3 style="padding-left: 42pt;text-indent: -24pt;line-height: 107%;text-align: justify;"><a name="bookmark3">	Joint Entity, Relation and Event Mention Extraction and Linking from Speech and Text</a></h3><p class="s8" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark5" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">For speech input, we apply the Amazon Transcribe API </a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">4</span> <span style=" color: #000;">for converting English and Spanish speech to text. When the language is not specified, it is automatically detected from the audio signal. It returns the transcription with starting and ending</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="80" height="1" alt="image" src="wen_files/Image_004.png"/></span></p><p class="s9" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark4">3</a><a href="https://www.darpa.mil/program/knowledge-directed-artificial-intelligence-reasoning-over-schemas" class="s11">https://www.darpa.mil/program/knowledge-di</a></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="https://www.darpa.mil/program/knowledge-directed-artificial-intelligence-reasoning-over-schemas" class="s11" target="_blank" name="bookmark5">rected-artificial-intelligence-reasoning-over- schemas</a></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="https://aws.amazon.com/transcribe/" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt;" target="_blank">4</a><a href="https://aws.amazon.com/transcribe/" class="s11" target="_blank">https://aws.amazon.com/transcribe/</a></p><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: justify;">times for each detected words, as well as potential alternative transcriptions.</p><p class="s13" style="padding-top: 4pt;padding-left: 12pt;text-indent: 11pt;line-height: 107%;text-align: justify;"><a href="#bookmark52" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Then from the speech recognition results and text input, we extract entity, relation, and event mentions using OneIE (</a><a href="#bookmark52" class="s7">Lin et </a><span class="s8">al.</span><a href="#bookmark52" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a><span class="s8">2020</span><span class="p">), a state- of-the-art joint neural model for sentence-level in- formation extraction. Given a sentence, the goal of this module is to extract an information graph </span>G <span class="s28">= (</span>V, E<span class="s28">)</span><span class="p">, where </span>V <span class="p">is the node set containing en- tity mentions and event triggers and </span>E <a href="#bookmark26" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">is the edge set containing entity relations and event-argument links. We use a pre-trained BERT encoder (</a><a href="#bookmark26" class="s7">De- vlin et </a><span class="s8">al.</span><a href="#bookmark26" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a><span class="s8">2018</span><span class="p">) to obtain contextualized word representations for the input sentence. Next, we adopt separate conditional random field-based tag- gers to identify entity mention and event trigger spans from the sentence. We represent each span, or node in the information graph, by averaging vectors of words in the span. After that, we calcu- late the label scores for each node or edge using separate task-specific feed forward networks. In order to capture the interactions among knowledge elements, we incorporate schema-guided global features when decoding information graphs. For a candidate graph </span>G<span class="p">, we define a global feature vector </span><span class="s29">f </span><span class="s28">= </span>{f<span class="s30">1</span><span class="s28">(</span>G<span class="s28">)</span>, ..., f<span class="s31">M</span> <span class="s28">(</span>G<span class="s28">)</span>}<span class="p">, where </span>f<span class="s31">i</span><span class="s28">(</span>·<span class="s28">) </span><span class="p">is a function that evaluates whether </span>G <span class="p">matches a spe- cific global feature. We compute the global feature score as </span><span class="s29">uf </span><span class="p">, where </span><span class="s29">u </span><span class="p">is a learnable weight vec- tor. Finally, we use a beam search-based decoder to generate the information graph with the highest global score. After we extract these mentions, we</span></p><p class="s8" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark38" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">apply a syntactic parser (</a>Honnibal et al.<a href="#bookmark38" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark65" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (</a><a href="#bookmark65" class="s7">Pan et </a>al.<a href="#bookmark65" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2017<a href="#bookmark74" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) to link entity mentions to WikiData (</a><a href="#bookmark74" class="s7">Vran- decˇic´ </a>and Krötzsch<a href="#bookmark74" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2014<a href="#bookmark8" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">)</a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">5</span><span style=" color: #000;">.</span></p></li><li data-list-text="2.3"><h3 style="padding-top: 10pt;padding-left: 42pt;text-indent: -24pt;line-height: 107%;text-align: left;"><a name="bookmark6">Document-level Event Argument Extraction</a></h3><p style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: right;"><a href="#bookmark51" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">The previous module can only operate on the sen- tence level. In particular, event arguments can of- ten be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (</a><a href="#bookmark51" class="s7">Li et </a><span style=" color: #00007F;">al.</span><a href="#bookmark51" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a><span style=" color: #00007F;">2021</span>) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as <i>conditional text generation</i>. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank <i>event template</i>. For example, the template for <span class="s32">Transportation </span>event type is <i>arg1 transported arg2 in arg3 from arg4 place to arg5 place</i><a href="#bookmark46" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. The de- sired output is a filled template with the arguments. Our model is based on BART (</a><a href="#bookmark46" class="s7">Lewis et </a><span style=" color: #00007F;">al.</span><a href="#bookmark46" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a><span style=" color: #00007F;">2020</span>), which is an encoder-decoder language model. To utilize the encoder-decoder LM for argu- ment extraction, we construct an input sequence of</p><p class="s13" style="padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">(<span class="p">s</span>) <span class="p">template </span>(s)(/s)<span class="p">document </span>(/s)<span class="p">. All argument names (arg1, arg2 etc.) in the template are replaced by a special placeholder token </span>(<span class="p">arg</span>)<span class="p">. This model is trained in an end-to-end fashion by directly opti- mizing the generation probability.</span></p><p style="padding-left: 17pt;text-indent: 10pt;line-height: 107%;text-align: justify;">To align the extracted arguments back to the document, we adopt a simple postprocessing pro- cedure and find the matching text span closest to the corresponding event trigger.</p></li><li data-list-text="2.4"><h3 style="padding-top: 10pt;padding-left: 42pt;text-indent: -24pt;line-height: 107%;text-align: justify;"><a name="bookmark7">Cross-document Cross-lingual Entity and Event Coreference Resolution</a></h3><p class="s8" style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark45" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">After extracting all mentions of entities and events, we apply our cross-document cross-lingual entity coreference resolution model, which is an exten- sion of the e2e-coref model (</a><a href="#bookmark45" class="s7">Lee et </a>al.<a href="#bookmark45" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2017<a href="#bookmark25" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). We use the multilingual XLM-RoBERTa (XLM- R) Transformer model (</a><a href="#bookmark25" class="s7">Conneau et </a>al.<a href="#bookmark25" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<span style=" color: #000;">) so that our coreference resolution model can handle non-English data. Second, we port the e2e-coref model to the </span><span class="s13">cross-lingual cross-document </span><span style=" color: #000;">setting.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="80" height="1" alt="image" src="wen_files/Image_005.png"/></span></p><p class="s9" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a name="bookmark8">5</a><a href="https://www.wikidata.org/" class="s11">https://www.wikidata.org/</a></p><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Given <i>N </i>hybrid English and Spanish input docu- ments, we create <u><i>N</i></u><u>(</u><u><i>N</i></u><span class="s35">−</span><u>1)</u> pairs of documents and treat each pair as a single “mega-document”. We apply our model to each mega-document and, at the end, aggregate the predictions across all mega- documents to extract the coreference clusters. Fi- nally, we also apply a simple heuristic rule that prevents two entity mentions from being merged together if they are linked to different entities with high confidence.</p><p class="s36" style="text-indent: 0pt;line-height: 8pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s8" style="padding-top: 1pt;padding-left: 13pt;text-indent: 10pt;line-height: 107%;text-align: justify;"><a href="#bookmark44" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Our event coreference resolution method (</a><a href="#bookmark44" class="s7">Lai et </a>al.<a href="#bookmark44" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2021<span style=" color: #000;">) is similar to entity coreference res- olution, while incorporating additional symbolic features such as the event type information. If the input documents are all about one specific com- plex event, we apply some schema-guided heuristic rules to further refine the predictions of the neural event coreference resolution model. For example, in a bombing schema, there is typically only one bombing event. Therefore, in a document cluster, if there are two event mentions of type </span><span class="s13">bombing </span><span style=" color: #000;">and they have several arguments in common, these two mentions will be considered as coreferential.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.5"><h3 style="padding-left: 37pt;text-indent: -24pt;line-height: 107%;text-align: justify;"><a name="bookmark9">Cross-document Temporal Event Ordering</a></h3><p class="s8" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark82" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Based on the event coreference resolution compo- nent described above, we group all mentions into clusters. Next we aim to order events along a time- line. We follow </a><a href="#bookmark82" class="s7">Zhou et </a>al. <a href="#bookmark82" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">(</a>2020<a href="#bookmark68" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) to design a component for temporal event ordering. Specifi- cally, we further pre-train a T5 model (</a><a href="#bookmark68" class="s7">Raffel et </a>al.<a href="#bookmark68" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) with distant temporal ordering supervision signals. These signals are acquired through two set of syntactic patterns: 1) before/after keywords in text and 2) explicit date and time mentions. We take such a pre-trained temporal T5 model and fine- tune it on MATRES (</a><a href="#bookmark63" class="s7">Ning et </a>al.<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018b<span style=" color: #000;">) and use it as the system for temporal event ordering. We perform pair-wise temporal relation classification for all event mention pairs in a documents.</span></p><p class="s8" style="padding-top: 1pt;padding-left: 12pt;text-indent: 11pt;line-height: 107%;text-align: justify;"><a href="#bookmark53" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">We further train an alternative model from fine- tuning RoBERTa (</a><a href="#bookmark53" class="s7">Liu et </a>al.<a href="#bookmark53" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) on MATRES (</a><a href="#bookmark63" class="s7">Ning et </a>al.<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018b<a href="#bookmark77" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). This model has also been suc- cessfully applied for event time prediction (</a><a href="#bookmark77" class="s7">Wen et </a>al.<a href="#bookmark77" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2021<a href="#bookmark48" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark48" class="s7">Li et </a>al.<a href="#bookmark48" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020a<span style=" color: #000;">). We only consider event mention pairs which are within neighboring sentences, or can be connected by shared argu- ments.</span></p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 10pt;line-height: 107%;text-align: justify;">Besides model prediction, we also learn high confident patterns from the schema repository. We</p><p style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">consider temporal relations that appear very fre- quently as our prior knowledge. For each given document cluster, we apply these patterns as high- precision patterns before two statistical temporal ordering models separately. The schema matching algorithm will select the best matching from two graphs as the final instantiated schema results.</p><p class="s8" style="padding-top: 2pt;padding-left: 17pt;text-indent: 10pt;line-height: 107%;text-align: justify;"><a href="#bookmark11" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Because the annotation for non-English data can be expensive and time-consuming, the temporal event tracking component has only been trained on English input. To extend the temporal event tracking capability to cross-lingual setting, we ap- ply Google Cloud neural machine translation </a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">6</span> <a href="#bookmark31" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">to translate Spanish documents into English and ap- ply the FastAlign algorithm (</a>Dyer et al.<a href="#bookmark31" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2013<span style=" color: #000;">) to obtain word alignment.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.6"><h3 style="padding-left: 42pt;text-indent: -24pt;line-height: 107%;text-align: left;"><a name="bookmark10">Cross-media Information Grounding and Fusion</a></h3><h3 style="padding-top: 8pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Visual event and argument role extraction: <span class="p">Our goal is to extract visual events along with their argument roles from visual data, i.e., images and videos. In order to train event extractor from vi- sual data, we have collected a new dataset called </span>Video M2E2 <span class="p">which contains 1,500 video-article pairs by searching over YouTube news channels using 18 event primitives related to visual concepts as search keywords. We have extensively anno- tated the the videos and sampled key frames for annotating bounding boxes of argument roles.</span></h3><p class="s8" style="padding-top: 2pt;padding-left: 17pt;text-indent: 11pt;line-height: 107%;text-align: justify;"><a href="#bookmark37" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Our Visual Event and Argument Role Extraction system consists of an event classification model (ResNet-50 (</a><a href="#bookmark37" class="s7">He et </a>al.<a href="#bookmark37" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">)) and an argument role extraction model (JSL (</a><a href="#bookmark57" class="s7">Marasovic´ et </a>al.<a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">)). To extract the events and associated argument roles, we leverage a public dataset called Situation with Groundings (SWiG) (</a><a href="#bookmark57" class="s7">Marasovic´ et </a>al.<a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<span style=" color: #000;">) to pretrain our system. SWiG is designed for event and argument understanding in images with ob- ject groundings but has a different ontology. We mapped the event types, argument role types and entity names in SWiG to our ontology (covering 12 event sub-types) so that our model is able to extract event information from both images and videos. For videos, we sample frames at a frame rate of 1 frame per second and process them as individual images. In this way, we have a unified model for both image and video inputs.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="80" height="1" alt="image" src="wen_files/Image_006.png"/></span></p><p class="s9" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark11">6</a><a href="https://cloud.google.com/translate/docs/advanced/translating-text-v3" class="s11">https://cloud.google.com/translate/docs/ad</a></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="https://cloud.google.com/translate/docs/advanced/translating-text-v3" class="s11">vanced/translating-text-v3</a></p><p class="s8" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span class="h3">Multimodal event coreference:  </span><a href="#bookmark21" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">We further ex- tended the previous visual event extraction model to find coreference links between visual and text events. For the video frames with detected events, we apply a weakly-supervised grounding model (</a><a href="#bookmark21" class="s7">Akbari et </a>al.<a href="#bookmark21" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<span style=" color: #000;">) to find sentences and video frames that have high frame-to-sentence similar- ity, representing the sentence content similar to the video frame content. We apply a rule-based approach to determine if a visual event mention and a textual event mention are coreferential: (1) Their event types match; (2) No contradiction in the entity types for the same argument role across different modalities. (3) The video frame and sen- tence have a high semantic similarity score. Based on this pipeline, we are able to add visual prove- nance of events into the event graph. Moreover, we are able to add visual-only arguments to the event graph, which makes the event graph more informative.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.7"><h3 style="padding-left: 37pt;text-indent: -24pt;text-align: left;"><a name="bookmark12">Schema Matching</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 12pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark50" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Once we have acquired a large-scale schema reposi- tory by schema induction methods (</a><a href="#bookmark50" class="s7">Li et </a>al.<a href="#bookmark50" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020c<span style=" color: #000;">), we can view it as providing a scaffolding that we can instantiate with incoming data to construct tem- poral event graphs. Based on each document clus- ter, we need to find the most accurate schema from the schema repository. We further design a schema matching algorithm that can align our extracted event, entities and relations to a schema.</span></p><p style="padding-top: 6pt;padding-left: 13pt;text-indent: 10pt;line-height: 107%;text-align: justify;">We first perform topological sort for events based on temporal relations for both IE graph and schema graph so that we can get linearized event sequences in chronological order. Then for each pair of IE graph and schema graph, we apply the longest common subsequence (LCS) method to find the best matching. Our schema matching con- siders coreference and relations, which will break the optimal substructure when only considering event sequences. We extend the algorithm by re- placing the best results for subproblems with a beam of candidates with ranking from a scoring metric that considers matched events, arguments and relations. The candidates consist of matched event pairs, and then we greedily match their argu- ments and relations for scoring. We merge the best matched IE graph and schema graph to form the final instantiated schema.</p></li></ol></li><li data-list-text="3"><h2 style="padding-top: 4pt;padding-left: 35pt;text-indent: -17pt;text-align: justify;">Experiments</h2><ol id="l3"><li data-list-text="3.1"><h3 style="padding-top: 9pt;padding-left: 42pt;text-indent: -24pt;text-align: justify;">Data</h3><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">We have conducted evaluations including schema matching and schema-guided information extrac- tion.</p></li><li data-list-text="3.2"><h3 style="padding-top: 11pt;padding-left: 42pt;text-indent: -24pt;text-align: justify;">Quantitative Performance</h3><p class="s8" style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><span class="h3">Schema Induction. </span><span style=" color: #000;">To induce schemas, we col- lect Wikipedia articles describing complex events related to </span><span class="s13">improvised explosive device (IED)</span><a href="#bookmark14" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, and extract event graphs by applying our IE system. The data statistics are shown in Table </a>1<a href="#bookmark50" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. We induce schemas by applying the path language model (</a><a href="#bookmark50" class="s7">Li et </a>al.<a href="#bookmark50" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020c<span style=" color: #000;">) over event paths in the training data,</span></p><p class="s8" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">2006<a href="#bookmark18" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">), EDL 2016</a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">7</span><a href="#bookmark19" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, EDL 2017</a><span style=" color: #00007F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt;">8</span><a href="#bookmark67" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, OntoNotes (</a><a href="#bookmark67" class="s7">Prad-</a></p><p class="s8" style="padding-left: 12pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark67" class="s7">han et </a>al.<a href="#bookmark67" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2012<a href="#bookmark72" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">), ERE (</a><a href="#bookmark72" class="s7">Song et </a>al.<a href="#bookmark72" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2015<a href="#bookmark73" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">), CoNLL 2002 (</a><a href="#bookmark73" class="s7">Tjong Kim </a>Sang<a href="#bookmark73" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2002<a href="#bookmark27" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">), DCEP (</a>Dias<a href="#bookmark27" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark70" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) and SemEval 2010 (</a><a href="#bookmark70" class="s7">Recasens et </a>al.<a href="#bookmark70" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2010<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">); tempo- ral ordering component on MATRES (</a><a href="#bookmark63" class="s7">Ning et </a>al.<a href="#bookmark63" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018b<a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">); visual event and argument extraction on Video M2E2 and SWiG (</a><a href="#bookmark57" class="s7">Marasovic´ et </a>al.<a href="#bookmark57" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark17" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). The statistics of our output are shown in Table </a>5<span style=" color: #000;">. The DARPA program’s phrase 1 human assess- ment on about 25% of our system output shows that about 70% of events are correctly extracted.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="286" height="1" alt="image" src="wen_files/Image_007.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="288" height="1" alt="image" src="wen_files/Image_008.png"/></span></p><h4 style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a name="bookmark13">Component        Benchmark  Metric Score</a></h4><p class="s37" style="padding-top: 6pt;padding-left: 83pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Trigger  ACE+ERE   F<span class="s38">1   </span>64.1</p><p class="s37" style="padding-left: 64pt;text-indent: 0pt;line-height: 11pt;text-align: left;">En Argument  ACE+ERE   F<span class="s38">1   </span>49.7</p><p class="s8" style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark15" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">and merge top ranked paths into schema graphs for human curation. The statistics of the human curated schema repository are shown in Table </a>2<span style=" color: #000;">.</span></p><p class="s10" style="padding-left: 17pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Mention</p><p class="s10" style="padding-left: 17pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Extraction</p><p class="s39" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">  Relation  ACE+ERE &nbsp; F<span class="s40">1 &nbsp; </span>49.5 </p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:12pt"><td style="width:105pt"><p class="s41" style="padding-right: 5pt;text-indent: 0pt;line-height: 10pt;text-align: right;">Es Argument</p></td><td style="width:55pt"><p class="s41" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">ACE+ERE</p></td><td style="width:26pt"><p class="s42" style="padding-left: 10pt;text-indent: 0pt;line-height: 10pt;text-align: left;">F<span class="s43">1</span></p></td><td style="width:29pt"><p class="s41" style="padding-right: 4pt;text-indent: 0pt;line-height: 10pt;text-align: right;">46.0</p></td></tr><tr style="height:14pt"><td style="width:105pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-right: 7pt;text-indent: 0pt;line-height: 10pt;text-align: right;">Relation</p></td><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">ACE+ERE</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s42" style="padding-left: 10pt;text-indent: 0pt;line-height: 11pt;text-align: left;">F<span class="s43">1</span></p></td><td style="width:29pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-right: 4pt;text-indent: 0pt;line-height: 10pt;text-align: right;">46.6</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">Trigger  ACE+ERE   F<span class="s38">1   </span>63.4</p><p style="text-indent: 0pt;text-align: left;"><span><img width="291" height="1" alt="image" src="wen_files/Image_009.png"/></span></p><h4 style="padding-top: 15pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a name="bookmark14">Split #Docs  #Events  #Arguments  #Relations</a></h4><p class="s10" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 88%;text-align: left;">Document-level Argument Extraction</p><p class="s37" style="padding-top: 4pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">ACE    F<span class="s38">1   </span>66.7</p><p class="s37" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">RAMS   F<span class="s38">1   </span>48.6</p><p class="s5" style="padding-top: 47pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Table 1: Data statistics of IED Schema Learning Cor- pus.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 16pt;text-indent: 0pt;line-height: 106%;text-align: left;">Temporal Ordering</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 17pt;text-indent: 14pt;text-align: left;">Event   ERE-ES  CoNLL 81.0</p><p style="text-indent: 0pt;text-align: left;"><span><img width="288" height="1" alt="image" src="wen_files/Image_010.png"/></span></p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:12pt"><td style="width:61pt;border-top-style:solid;border-top-width:1pt"><p class="s44" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Train <span class="s41">5,247</span></p></td><td style="width:48pt;border-top-style:solid;border-top-width:1pt"><p class="s41" style="padding-top: 1pt;padding-left: 9pt;text-indent: 0pt;line-height: 10pt;text-align: left;">41,672</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt"><p class="s41" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;">136,894</p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt"><p class="s41" style="padding-top: 1pt;padding-left: 11pt;padding-right: 11pt;text-indent: 0pt;line-height: 10pt;text-align: center;">122,846</p></td><td style="width:19pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:215pt;border-top-style:solid;border-top-width:1pt"><p class="s45" style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;line-height: 8pt;text-align: left;">En  <span class="s41">Entity  OntoNotes CoNLL 92.4</span></p></td></tr><tr style="height:10pt"><td style="width:61pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Dev  <span class="s41">575</span></p></td><td style="width:48pt"><p class="s41" style="padding-left: 11pt;text-indent: 0pt;line-height: 9pt;text-align: left;">4,661</p></td><td style="width:55pt"><p class="s41" style="padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: left;">15,404</p></td><td style="width:54pt"><p class="s41" style="padding-left: 11pt;padding-right: 11pt;text-indent: 0pt;line-height: 9pt;text-align: center;">13,320</p></td><td style="width:19pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:215pt"><p class="s41" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Coreference <u>&nbsp;&nbsp; Event &nbsp; ACE &nbsp; CoNLL 84.8 </u></p></td></tr><tr style="height:13pt"><td style="width:61pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Test  <span class="s41">577</span></p></td><td style="width:48pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">5,089</p></td><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 15pt;text-indent: 0pt;line-height: 10pt;text-align: left;">16,721</p></td><td style="width:54pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 11pt;padding-right: 11pt;text-indent: 0pt;line-height: 10pt;text-align: center;">14,054</p></td><td style="width:19pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:215pt"><p class="s47" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Resolution <span class="s45">Es  </span><span class="s41">Entity SemEval 2010 CoNLL 67.6</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-top: 6pt;padding-left: 30pt;text-indent: -13pt;line-height: 106%;text-align: left;">RoBERTa   MATRES   F1  78.8 T5    MATRES-b  Acc.  89.6</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 254pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="285" height="1" alt="image" src="wen_files/Image_011.png"/></span></p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:25pt"><td style="width:79pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s44" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">Schema</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s44" style="padding-top: 1pt;padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">#Steps</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s44" style="padding-top: 1pt;padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">#Arguments</p></td><td style="width:52pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s44" style="padding-top: 2pt;padding-left: 14pt;text-indent: -9pt;text-align: left;">#Temporal Links</p></td></tr><tr style="height:13pt"><td style="width:79pt;border-top-style:solid;border-top-width:1pt"><p class="s44" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Disease Outbreak</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p class="s41" style="padding-top: 1pt;padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 10pt;text-align: center;">20</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt"><p class="s41" style="padding-top: 1pt;padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 10pt;text-align: center;">94</p></td><td style="width:52pt;border-top-style:solid;border-top-width:1pt"><p class="s41" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;">20</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Disaster Relief</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">15</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">85</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">15</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Medical Treatment</p></td><td style="width:31pt"><p class="s41" style="text-indent: 0pt;line-height: 9pt;text-align: center;">8</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">37</p></td><td style="width:52pt"><p class="s41" style="padding-left: 23pt;text-indent: 0pt;line-height: 9pt;text-align: left;">8</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Search and Rescue</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">11</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">50</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">10</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">General Attack</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">21</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">89</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">27</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">General IED</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">33</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">144</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">43</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Roadside IED</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">28</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">123</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">36</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Car IED</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">34</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">148</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">45</p></td></tr><tr style="height:10pt"><td style="width:79pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Drone Strikes IED</p></td><td style="width:31pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">32</p></td><td style="width:56pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">142</p></td><td style="width:52pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">48</p></td></tr><tr style="height:13pt"><td style="width:79pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s44" style="padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Backpack IED</p></td><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 10pt;text-align: center;">31</p></td><td style="width:56pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 10pt;text-align: center;">138</p></td><td style="width:52pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s41" style="padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: left;">40</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-top: 2pt;padding-bottom: 2pt;padding-left: 256pt;text-indent: 0pt;text-align: left;"><a name="bookmark15">Visual Event Extraction   Video M2E2  Acc.  70.0</a></p><p style="padding-left: 254pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="286" height="1" alt="image" src="wen_files/Image_012.png"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 253pt;text-indent: 0pt;text-align: justify;">Table 3:  Performance (%) of each component. MATRES-b refers to MATRES binary classification that only considers BEFORE and AFTER relations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s48" style="padding-left: 253pt;text-indent: 0pt;text-align: left;"><a name="bookmark16"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;</a></p><h4 style="padding-top: 3pt;padding-left: 259pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Category  Complex</h4><h4 style="padding-left: 308pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Events</h4><h4 style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Documents Images  Videos</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">Table 2: Data statistics of the induced schema library.</p><p class="s48" style="padding-left: 17pt;text-indent: 0pt;line-height: 9pt;text-align: left;"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;</p><p class="s10" style="padding-bottom: 3pt;padding-left: 23pt;text-indent: 0pt;line-height: 10pt;text-align: left;">#       11    139    1,213    31</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="289" height="1" alt="image" src="wen_files/Image_013.png"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Table 4: Data statistics for schema matching corpus (LDC2020E39).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: left;">Schema-guided Information Extraction.  <span class="p">The performance of each component is shown in</span></h3><h4 style="padding-top: 9pt;padding-left: 17pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a name="bookmark17">Category  Extracted</a></h4><p style="text-indent: 0pt;text-align: left;"><span><img width="291" height="1" alt="image" src="wen_files/Image_014.png"/></span></p><h4 style="padding-left: 74pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Events</h4><h4 style="padding-top: 9pt;padding-left: 22pt;text-indent: -4pt;text-align: left;">Schema Steps</h4><h4 style="padding-top: 9pt;padding-left: 29pt;text-indent: -12pt;text-align: left;">Instantiated Steps</h4><p class="s8" style="padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><a href="#bookmark13" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Table </a>3<a href="#bookmark16" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">.  We evaluate the end-to-end perfor- mance of our system on a complex event cor- pus (LDC2020E39), which contains multi-lingual multi-media document clusters. The data statistics are shown in Table </a>4<a href="#bookmark76" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. We train our mention ex- traction component on ACE 2005 (</a><a href="#bookmark76" class="s7">Walker et </a>al.<a href="#bookmark76" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2006<a href="#bookmark72" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) and ERE (</a><a href="#bookmark72" class="s7">Song et </a>al.<a href="#bookmark72" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2015<a href="#bookmark76" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">); document- level argument exraction on ACE 2005 (</a><a href="#bookmark76" class="s7">Walker et </a>al.<a href="#bookmark76" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2006<a href="#bookmark32" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) and RAMS (</a><a href="#bookmark32" class="s7">Ebner et </a>al.<a href="#bookmark32" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark76" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">); coref- erence component on ACE 2005 (</a><a href="#bookmark76" class="s7">Walker et </a>al.<span style=" color: #000;">,</span></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="293" height="1" alt="image" src="wen_files/Image_015.png"/></span></p><h4 style="padding-top: 1pt;padding-bottom: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">#       <span class="s10">3,180     1,738      958</span></h4><p style="padding-left: 13pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="289" height="1" alt="image" src="wen_files/Image_016.png"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 47pt;text-indent: 0pt;text-align: left;">Table 5: Results of schema matching.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.3"><h3 style="padding-top: 9pt;padding-left: 37pt;text-indent: -24pt;text-align: left;">Qualitative Analysis</h3><p class="s8" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: left;"><a href="#bookmark20" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Figure </a>2 <span style=" color: #000;">illustrates a subset of examples for the best matched results from our end-to-end system. We</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="80" height="1" alt="image" src="wen_files/Image_017.png"/></span></p><p class="s9" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a name="bookmark18">7</a><span class="s10">LDC2017E03</span><a name="bookmark19">&zwnj;</a></p><p class="s9" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">8<span class="s10">LDC2017E52</span></p><p class="s49" style="padding-top: 3pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a name="bookmark20">Extracted Graph</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="511" height="155" alt="image" src="wen_files/Image_018.png"/></span></p><p class="s50" style="text-indent: 0pt;text-align: right;">Old Bailey</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s50" style="padding-left: 24pt;text-indent: 0pt;text-align: left;">A court in British legal history</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s50" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">Max Hill</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s50" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">Manchester</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s51" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">... ...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Place</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">JudgeCourt</p><p class="s52" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">JudgeCourt</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s53" style="text-indent: 0pt;text-align: left;">Broadcast</p><p class="s52" style="padding-top: 4pt;text-indent: 11pt;line-height: 280%;text-align: left;">Communicator JudgeCourt</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-top: 6pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Place</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">Resident</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">ReleaseParole</p><p class="s15" style="padding-top: 1pt;padding-left: 42pt;text-indent: 0pt;text-align: left;">ChargeIndict     TrialHearing      Sentence     ArrestJailDetain    ReleaseParole</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">Defendant</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Defendant</p><p class="s52" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Defendant</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 36pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Detainee   Defendant</p><p class="s52" style="padding-left: 141pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Defendant</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="213" height="2" alt="image" src="wen_files/Image_019.png"/></span></p><p class="s49" style="padding-left: 23pt;text-indent: 0pt;text-align: left;">Schema</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 23pt;text-indent: 0pt;text-align: left;">Defendant</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s55" style="padding-left: 23pt;text-indent: -2pt;line-height: 160%;text-align: center;">Salman Abedi <span class="s56">Temporal Ordering </span>Attacker</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s57" style="padding-left: 4pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span><img width="5" height="2" alt="image" src="wen_files/Image_020.png"/></span> <span><img width="255" height="8" alt="image" src="wen_files/Image_021.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-top: 6pt;padding-left: 45pt;text-indent: 0pt;text-align: left;">Defendant</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s51" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 7pt;text-align: left;">... ...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Defendant</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">Defendant   Detainee</p><p class="s15" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">ChargeIndict</p><p class="s15" style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">TrialHearing</p><p class="s52" style="padding-left: 15pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Defendant</p><p class="s15" style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Sentence</p><p class="s15" style="padding-top: 4pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">ArrestJailDetain   ReleaseParole</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-top: 4pt;padding-left: 31pt;text-indent: 0pt;text-align: center;">Defendant</p><p class="s53" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: center;">Convict</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="441" height="138" alt="image" src="wen_files/Image_022.png"/></span></p><p class="s53" style="padding-top: 4pt;padding-left: 159pt;text-indent: 0pt;text-align: left;">ReleaseParole</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements.</p></li></ol></li><li data-list-text="4"><h2 style="padding-top: 11pt;padding-left: 35pt;text-indent: -17pt;text-align: left;">Related Work</h2><p class="s8" style="padding-top: 9pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span class="h3">Text Information Extraction.  </span><a href="#bookmark75" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Existing end-to- end Information Extraction (IE) systems (</a><a href="#bookmark75" class="s7">Wadden et </a>al.<a href="#bookmark75" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<a href="#bookmark49" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark49" class="s7">Li et </a>al.<a href="#bookmark49" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020b<a href="#bookmark52" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark52" class="s7">Lin et </a>al.<a href="#bookmark52" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark47" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark47" class="s7">Li et </a>al.<a href="#bookmark47" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<span style=" color: #000;">) mainly focus on extracting entities, events and entity relations from individual sen- tences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict fu- ture events.</span></p><p class="s8" style="padding-top: 9pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span class="h3">Multimedia Information Extraction.  </span><a href="#bookmark49" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Previous multimedia IE systems (</a><a href="#bookmark49" class="s7">Li et </a>al.<a href="#bookmark49" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020b<a href="#bookmark79" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark79" class="s7">Yazici et </a>al.<a href="#bookmark79" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018<span style=" color: #000;">) only include cross-media entity coref- erence resolution by grounding the extracted visual entities to text. We are the first to perform cross- media joint event extraction and coreference reso- lution to obtain the coreferential events from text, images and videos.</span></p><p class="s8" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span class="h3">Coreference Resolution.  </span><a href="#bookmark59" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Previous neural mod- els for event coreference resolution use non- contextual (</a><a href="#bookmark59" class="s7">Nguyen et </a>al.<a href="#bookmark59" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark24" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark24" class="s7">Choubey et </a>al.<a href="#bookmark24" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark39" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark39" class="s7">Huang et </a>al.<a href="#bookmark39" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<a href="#bookmark56" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) or contextual word repre- sentations (</a><a href="#bookmark56" class="s7">Lu et </a>al.<a href="#bookmark56" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark80" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark80" class="s7">Yu et </a>al.<a href="#bookmark80" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2020<a href="#bookmark22" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">). We in- corporate a wide range of symbolic features (</a><a href="#bookmark22" class="s7">Chen and </a>Ji<a href="#bookmark22" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2009<a href="#bookmark23" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark23" class="s7">Chen et </a>al.<a href="#bookmark23" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2009<a href="#bookmark71" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark71" class="s7">Sammons et </a>al.<a href="#bookmark71" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2015<a href="#bookmark54" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark54" class="s7">Lu and </a>Ng<a href="#bookmark54" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2016<a href="#bookmark55" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2017<a href="#bookmark30" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark30" class="s7">Duncan et </a>al.<a href="#bookmark30" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2017<span style=" color: #000;">), such as event attributes and types, into our event coreference resolution module using a context- dependent gate mechanism.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: justify;"><span class="h3">Temporal Event Ordering.  </span><a href="#bookmark60" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Temporal relations between events are extracted for neighbor events in one sentence (</a>Ning et al.<a href="#bookmark60" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2017<a href="#bookmark61" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2018a<a href="#bookmark62" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<a href="#bookmark36" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">; </a><a href="#bookmark36" class="s7">Han et </a>al.<a href="#bookmark36" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">, </a>2019<span style=" color: #000;">), ignoring the temporal dependen- cies between events across sentences. We perform document-level event ordering and propagate tem- poral attributes through shared arguments. Further- more, we take advantage of the schema repository knowledge by using the frequent temporal order between event types to guide the ordering between events.</span></p></li><li data-list-text="5"><h2 style="padding-top: 4pt;padding-left: 35pt;text-indent: -17pt;text-align: left;">Conclusions and Future Work</h2><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">We demonstrate a state-of-the-art schema-guided cross-document cross-lingual cross-media informa- tion extraction and event tracking system. This system is made publicly available to enable users to effectively harness rich information from a va- riety of sources, languages and modalities. In the future, we plan to develop more advanced graph neural networks based method for schema match- ing and schema-guided event prediction.</p></li><li data-list-text="6"><h2 style="padding-top: 9pt;padding-left: 35pt;text-indent: -17pt;text-align: left;">Broader Impact</h2></li></ol><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 0pt;line-height: 107%;text-align: justify;">Our goal in developing Cross-document Cross- lingual Cross-media information extraction and event tracking systems is to advance the state-of- the-art and enhance the field’s ability to fully un- derstand real-world events from multiple sources, languages and modalities. We believe that to make real progress in event-centric Natural Language Un- derstanding, we should not focus only on datasets, but to also ground our work in real-world applica- tions. The application we focus on is navigating news, and the examples shown here and in the paper demonstrate the potential use in news under- standing.</p><p style="padding-left: 17pt;text-indent: 11pt;line-height: 107%;text-align: justify;">For our demo, the distinction between benefi- cial use and harmful use depends, in part, on the data. Proper use of the technology requires that input documents/images are legally and ethically obtained. We are particularly excited about the potential use of the technologies in applications of broad societal impact, such as disaster moni- toring and emergency response. Training and as- sessment data is often biased in ways that limit system accuracy on less well represented popula- tions and in new domains. The performance of our system components as reported in the experiment section is based on the specific benchmark datasets, which could be affected by such data biases. Thus questions concerning generalizability and fairness should be carefully considered.</p><p style="padding-left: 17pt;text-indent: 10pt;line-height: 107%;text-align: justify;">A general approach to ensure proper, rather than malicious, application of dual-use technol- ogy should: incorporate ethics considerations as the first-order principles in every step of the system design, maintain a high degree of transparency and interpretability of data, algorithms, models, and functionality throughout the system. We intend to make our software available as open source and shared docker containers for public verification and auditing, and explore countermeasures to protect</p><p style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">vulnerable groups.</p><h2 style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Acknowledgement</h2><p style="padding-top: 7pt;padding-left: 13pt;text-indent: 0pt;line-height: 107%;text-align: left;">This research is based upon work supported in part by U.S. DARPA KAIROS Program No. FA8750-</p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 107%;text-align: justify;">19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copy- right annotation therein. We thank all the anno- tators who have contributed to the annotations of our training data for the joint IE component (in alphabetical order): Daniel Campos, Anthony Cuff, Yi R. Fung, Xiaodan Hu, Emma Bonnette Hamel, Samual Kriman, Meha Goyal Kumar, Manling Li, Tuan M. Lai, Ying Lin, Sarah Moeller, Ashley Nobi, Xiaoman Pan, Nikolaus Parulian, Adams Pollins, Rachel Rosset, Haoyu Wang, Qingyun Wang, Zhenhailong Wang, Spencer Whitehead, Lucia Yao, Pengfei Yu, Qi Zeng, Haoran Zhang, Hongming Zhang, Zixuan Zhang.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 13pt;text-indent: 0pt;text-align: left;">References</h2><p class="s5" style="padding-top: 6pt;padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a name="bookmark21">Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, and Shih-Fu Chang. 2019.  Multi-level multimodal common semantic space for image-phrase grounding. In </a><i>Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition</i>, pages 12476–12486.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark22">Zheng Chen and Heng Ji. 2009. Graph-based event coreference resolution. In </a><i>Proceedings of the 2009 Workshop on Graph-based Methods for Natural Lan- guage Processing (TextGraphs-4)</i>, pages 54–57.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark23">Zheng Chen, Heng Ji, and Robert M Haralick. 2009. A pairwise event coreference model, feature impact and evaluation for event coreference resolution. In </a><i>Proceedings of the workshop on events in emerging text types</i>, pages 17–22.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/2020.acl-main.478" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark24">Prafulla Kumar Choubey, Aaron Lee, Ruihong Huang, and Lu Wang. 2020.  </a><a href="https://doi.org/10.18653/v1/2020.acl-main.478" class="a" target="_blank">Discourse as a function of event: Profiling discourse structure in news arti- cles around the main </a><span style=" color: #00007F;">event</span>. In <i>Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics</i>, pages 5374–5386, Online. As- sociation for Computational Linguistics.</p><p class="s59" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/2020.acl-main.747" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark25">Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2020. </a><a href="https://doi.org/10.18653/v1/2020.acl-main.747" class="a" target="_blank">Unsupervised cross-lingual representation learning at </a>scale<span style=" color: #000;">.  In</span></p><p class="s58" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics<span class="s5">, pages 8440– 8451, Online. Association for Computational Lin- guistics.</span></p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark26">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. </a><i>arXiv preprint arXiv:1810.04805</i>.</p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark27">Francisco Dias. 2016. Multilingual Automated Text Anonymization. Msc dissertation, Instituto Superior Técnico, Lisbon, Portugal, May.</a></p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.3115/v1/D14-1148" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark28">Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. 2014. </a><a href="https://doi.org/10.3115/v1/D14-1148" class="a" target="_blank">Using structured events to predict stock price movement: </a><span style=" color: #00007F;">An empirical investigation</span>. In <i>Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, pages 1415–1425, Doha, Qatar. Association for Computa- tional Linguistics.</p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/C16-1201" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark29">Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. 2016. </a><a href="https://www.aclweb.org/anthology/C16-1201" class="a" target="_blank">Knowledge-driven event embedding for stock </a><span style=" color: #00007F;">prediction</span>. In <i>Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</i>, pages 2133–2142, Osaka, Japan. The COLING 2016 Organizing Com- mittee.</p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark30">Chase Duncan, Liang-Wei Chan, Haoruo Peng, Hao Wu, Shyam Upadhyay, Nitish Gupta, Chen-Tse Tsai, Mark Sammons, and Dan Roth. 2017. Ui ccg tac- kbp2017 submissions: Entity discovery and linking, and event nugget detection and co-reference.  In </a><i>TAC</i>.</p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark31">Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameteriza- tion of ibm model 2. In </a><i>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, pages 644–648.</p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark32">Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. 2020. Multi-sentence ar- gument linking. In </a><i>Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics</i>.</p><p class="s5" style="padding-top: 8pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/D19-5102" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark33">Liat Ein-Dor, Ariel Gera, Orith Toledo-Ronen, Alon Halfon, Benjamin Sznajder, Lena Dankin, Yonatan Bilu, Yoav Katz, and Noam Slonim. 2019. </a><a href="https://doi.org/10.18653/v1/D19-5102" class="a" target="_blank">Finan- cial event extraction using Wikipedia-based weak </a><span style=" color: #00007F;">supervision</span>.  In <i>Proceedings of the Second Work- shop on Economics and Natural Language Process- ing</i>, pages 10–15, Hong Kong. Association for Com- putational Linguistics.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1023_Paper.pdf" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark34">Goran Glavaš, Jan Šnajder, Marie-Francine Moens, and Parisa Kordjamshidi. 2014. </a><a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1023_Paper.pdf" class="a" target="_blank">HiEve: A corpus for extracting event hierarchies from news </a><span style=" color: #00007F;">stories</span>. In <i>Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</i>, pages 3678–3683, Reykjavik, Iceland. European Language Resources Association (ELRA).</p><p class="s5" style="padding-top: 4pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/R13-2011" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark35">Goran Glavaš and Sanja Štajner. 2013. </a><a href="https://www.aclweb.org/anthology/R13-2011" class="a" target="_blank">Event-centered simplification of news </a><span style=" color: #00007F;">stories</span>. In <i>Proceedings of the Student Research Workshop associated with RANLP 2013</i>, pages 71–78, Hissar, Bulgaria. INCOMA Ltd. Shoumen, BULGARIA.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/D19-1041" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark36">Rujun Han, Qiang Ning, and Nanyun Peng. 2019. </a><a href="https://doi.org/10.18653/v1/D19-1041" class="a" target="_blank">Joint event and temporal relation extraction with </a><span style=" color: #00007F;">shared representations and structured prediction</span>. In <i>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</i>, pages 434–444. Association for Computational Linguistics.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark37">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In </a><i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pages 770– 778.</p><p class="s5" style="padding-top: 8pt;padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.5281/zenodo.1212303" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark38">Matthew Honnibal, Ines Montani, Sofie Van Lan- deghem,  and Adriane Boyd. 2020.   </a><a href="https://doi.org/10.5281/zenodo.1212303" class="a" target="_blank">spaCy: Industrial-strength Natural Language Processing in </a><span style=" color: #00007F;">Python</span>. <i>Zenodo</i>.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark39">Yin Jou Huang, Jing Lu, Sadao Kurohashi, and Vincent Ng. 2019. Improving event coreference resolution by learning argument compatibility from unlabeled data. In </a><i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers)</i>, pages 785– 795.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/W18-3101" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark40">Gilles Jacobs, Els Lefever, and Véronique Hoste. 2018. </a><a href="https://doi.org/10.18653/v1/W18-3101" class="a" target="_blank">Economic event detection in company-specific news </a><span style=" color: #00007F;">text</span>. In <i>Proceedings of the First Workshop on Eco- nomics and Natural Language Processing</i>, pages 1– 10, Melbourne, Australia. Association for Computa- tional Linguistics.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/N16-1056" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark41">Abhyuday N Jagannatha and Hong Yu. 2016.  </a><a href="https://doi.org/10.18653/v1/N16-1056" class="a" target="_blank">Bidi- rectional RNN for medical event detection in elec- tronic health </a><span style=" color: #00007F;">records</span>. In <i>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, pages 473–482, San Diego, California. Association for Computational Linguis- tics.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/W18-5620" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark42">Serena Jeblee and Graeme Hirst. 2018. </a><a href="https://doi.org/10.18653/v1/W18-5620" class="a" target="_blank">Listwise tempo- ral ordering of events in clinical </a><span style=" color: #00007F;">notes</span>. In <i>Proceed- ings of the Ninth International Workshop on Health Text Mining and Information Analysis</i>, pages 177– 182, Brussels, Belgium. Association for Computa- tional Linguistics.</p><p class="s5" style="padding-top: 8pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/L16-1545" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark43">Prescott Klassen, Fei Xia, and Meliha Yetisgen. 2016. </a><a href="https://www.aclweb.org/anthology/L16-1545" class="a" target="_blank">Annotating and detecting medical events in clin- ical </a><span style=" color: #00007F;">notes</span>.  In <i>Proceedings of the Tenth Inter- national Conference on Language Resources and Evaluation (LREC’16)</i>, pages 3417–3421, Portorož, Slovenia. European Language Resources Associa- tion (ELRA).</p><p class="s5" style="padding-top: 4pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark44">Tuan Lai, Heng Ji, Trung Bui, Quan Hung Tran, Franck Dernoncourt, and Walter Chang. 2021. A context- dependent gated module for incorporating symbolic semantics into event coreference resolution.  In </a><i>Proc. The 2021 Conference of the North American Chapter of the Association for Computational Lin- guistics - Human Language Technologies (NAACL- HLT2021)</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/D17-1018" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark45">Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. </a><a href="https://doi.org/10.18653/v1/D17-1018" class="a" target="_blank">End-to-end neural coreference reso- </a><span style=" color: #00007F;">lution</span>. In <i>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</i>, pages 188–197, Copenhagen, Denmark. Association for Computational Linguistics.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/2020.acl-main.703" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark46">Mike Lewis,  Yinhan Liu,  Naman Goyal,  Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. </a><a href="https://doi.org/10.18653/v1/2020.acl-main.703" class="a" target="_blank">BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, </a><span style=" color: #00007F;">and comprehension</span>. In <i>Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics</i>, pages 7871–7880, Online. Association for Computational Linguistics.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark47">Manling Li, Ying Lin, Joseph Hoover, Spencer White- head, Clare Voss, Morteza Dehghani, and Heng Ji. 2019. Multilingual entity, relation, event and human value extraction. In </a><i>Proceedings of the 2019 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics (Demonstra- tions)</i>, pages 110–115.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark48">Manling Li, Ying Lin, Tuan Manh Lai, Xiaoman Pan, Haoyang Wen, Sha Li, Zhenhailong Wang, Pengfei Yu, Lifu Huang, Di Lu, Qingyun Wang, Haoran Zhang, Qi Zeng, Chi Han, Zixuan Zhang, Yujia Qin, Xiaodan Hu, Nikolaus Parulian, Daniel Campos, Heng Ji, Brian Chen, Xudong Lin, Alireza Zareian, Amith Ananthram, Emily Allaway, Shih-Fu Chang, Kathleen McKeown, Yixiang Yao, Michael Spec- tor, Mitchell DeHaven, Daniel Napierski, Marjorie Freedman, Pedro Szekely, Haidong Zhu, Ram Neva- tia, Yang Bai, Yifan Wang, Ali Sadeghian, Haodi Ma, and Daisy Zhe Wang. 2020a.  GAIA at SM- KBP 2020 - a dockerlized multi-media multi-lingual knowledge extraction, clustering, temporal tracking and hypothesis generation system. In </a><i>Proceedings of Thirteenth Text Analysis Conference (TAC 2020)</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/2020.acl-demos.11" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark49">Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, Daniel Napierski, and Marjorie Freedman. 2020b. </a><a href="https://doi.org/10.18653/v1/2020.acl-demos.11" class="a" target="_blank">GAIA: A fine-grained multimedia knowledge extraction </a><span style=" color: #00007F;">system</span>. In <i>Pro- ceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics:  System Demonstrations</i>, pages 77–86, Online. Association for Computational Linguistics.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark50">Manling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers, and Clare Voss. 2020c.  Connecting the dots: Event graph schema induction with path language modeling. In</a></p><p class="s58" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Proc. The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP2020)<span class="s5">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark51">Sha Li, Heng Ji, and Jiawei Han. 2021. Document- level event argument extraction by conditional gen- eration. In </a><i>Proc. The 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics - Human Language Technologies (NAACL-HLT2021)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark52">Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020. A joint end-to-end neural model for information ex- traction with global features. In </a><i>Proc. The 58th An- nual Meeting of the Association for Computational Linguistics (ACL2020)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a href="http://arxiv.org/abs/1907.11692" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark53">Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. </a><a href="http://arxiv.org/abs/1907.11692" class="a" target="_blank">Roberta: A robustly optimized BERT pretraining ap- </a><span style=" color: #00007F;">proach</span>. <i>CoRR</i>, abs/1907.11692.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark54">Jing Lu and Vincent Ng. 2016.  Event coreference resolution with multi-pass sieves. In </a><i>Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</i>, pages 3996– 4003.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark55">Jing Lu and Vincent Ng. 2017.  Joint learning for event coreference resolution. In </a><i>Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers)</i>, pages 90–101.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a name="bookmark56">Yaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, and Le Sun. 2020. End-to-end neural event corefer- ence resolution. </a><i>arXiv preprint arXiv:2009.08153</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a name="bookmark57">Ana Marasovic´, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A Smith, and Yejin Choi. 2020.   Natural language rationales with full- stack visual reasoning: From pixels to semantic frames to commonsense graphs.  </a><i>arXiv preprint arXiv:2010.07526</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/2020.findings-emnlp.344" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark58">Salvador Medina Maza, Evangelia Spiliopoulou, Ed- uard Hovy, and Alexander Hauptmann. 2020. </a><a href="https://doi.org/10.18653/v1/2020.findings-emnlp.344" class="a" target="_blank">Event- related bias removal for real-time disaster </a><span style=" color: #00007F;">events</span>. In <i>Findings of the Association for Computational Lin- guistics: EMNLP 2020</i>, pages 3858–3868, Online. Association for Computational Linguistics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark59">Thien Huu Nguyen, Adam Meyers, and Ralph Grish- man. 2016. New york university 2016 system for kbp event nugget: A deep learning approach. In </a><i>TAC</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/D17-1108" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark60">Qiang Ning, Zhili Feng, and Dan Roth. 2017. </a><a href="https://doi.org/10.18653/v1/D17-1108" class="a" target="_blank">A struc- tured learning approach to temporal relation extrac- </a><span style=" color: #00007F;">tion</span>.  In <i>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</i>, pages 1027–1037, Copenhagen, Denmark. Associa- tion for Computational Linguistics.</p><p class="s5" style="padding-top: 4pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/P18-1212" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark61">Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018a. </a><a href="https://doi.org/10.18653/v1/P18-1212" class="a" target="_blank">Joint reasoning for temporal and causal </a><span style=" color: #00007F;">relations</span>. In <i>Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers</i>, pages 2278–2288. Association for Computational Linguistics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/D19-1642" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark62">Qiang Ning, Sanjay Subramanian, and Dan Roth. 2019. </a><a href="https://doi.org/10.18653/v1/D19-1642" class="a" target="_blank">An improved neural baseline for temporal relation </a><span style=" color: #00007F;">extraction</span>. In <i>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</i>, pages 6202–6208. Association for Computational Linguistics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark63">Qiang Ning, H. Wu, and D. Roth. 2018b. A multi- axis annotation scheme for event temporal relations. </a><i>ArXiv</i>, abs/1804.07828.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.1016/j.asoc.2020.106384" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark64">Ahmet Murat Özbayoglu, Mehmet Ugur Gudelek, and Omer Berat Sezer. 2020. </a><a href="https://doi.org/10.1016/j.asoc.2020.106384" class="a" target="_blank">Deep learning for finan- cial applications : A </a><span style=" color: #00007F;">survey</span>. <i>Appl. Soft Comput.</i>, 93:106384.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark65">Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross- lingual name tagging and linking for 282 languages. In </a><i>Proc. the 55th Annual Meeting of the Association for Computational Linguistics (ACL2017)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s59" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.1145/2663792.2663794" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark66">Sandeep Panem, Manish Gupta, and Vasudeva Varma. 2014. </a><a href="https://doi.org/10.1145/2663792.2663794" class="a" target="_blank">Structured information extraction from nat- ural disaster events on </a>twitter<span style=" color: #000;">. In </span><span class="s58">Proceedings of the 5th International Workshop on Web-scale Knowl- edge Representation Retrieval &amp; Reasoning, Web- KR@CIKM 2014, Shanghai, China, November 3,</span></p><p class="s58" style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">2014<span class="s5">, pages 1–8. ACM.</span></p><p class="s59" style="padding-top: 10pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/W12-4501/" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark67">Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012.  </a><a href="https://www.aclweb.org/anthology/W12-4501/" class="a" target="_blank">Conll- 2012 shared task:  Modeling multilingual unre- stricted coreference in </a>ontonotes<span style=" color: #000;">.  In </span><span class="s58">Joint Con- ference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning - Proceedings of the Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes, EMNLP-CoNLL 2012, July 13, 2012,</span></p><p class="s58" style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">Jeju Island, Korea<span class="s5">, pages 1–40. ACL.</span></p><p class="s5" style="padding-top: 10pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark68">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, M. Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of trans- fer learning with a unified text-to-text transformer. </a><i>J. Mach. Learn. Res.</i>, 21:140:1–140:67.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/W12-2404" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark69">Preethi Raghavan, Eric Fosler-Lussier, and Albert Lai. 2012.  </a><a href="https://www.aclweb.org/anthology/W12-2404" class="a" target="_blank">Temporal classification of medical </a><span style=" color: #00007F;">events</span>. In <i>BioNLP: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing</i>, pages 29–37, Montréal, Canada. Association for Compu- tational Linguistics.</p><p class="s5" style="padding-top: 4pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/S10-1001/" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark70">Marta Recasens, Lluís Màrquez, Emili Sapena, M. An- tònia Martí, Mariona Taulé, Véronique Hoste, Mas- simo Poesio, and Yannick Versley. 2010. </a><a href="https://www.aclweb.org/anthology/S10-1001/" class="a" target="_blank">Semeval- 2010 task 1: Coreference resolution in multiple lan- </a><span style=" color: #00007F;">guages</span>.  In <i>Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval@ACL 2010, Uppsala University, Uppsala, Sweden, July 15-16, 2010</i>, pages 1–8. The Association for Com- puter Linguistics.</p><p class="s5" style="padding-top: 9pt;padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a name="bookmark71">Mark Sammons, Haoruo Peng, Yangqiu Song, Shyam Upadhyay, Chen-Tse Tsai, Pavankumar Reddy, Subhro Roy, and Dan Roth. 2015. Illinois ccg tac 2015 event nugget, entity discovery and linking, and slot filler validation systems. In </a><i>TAC</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark72">Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, and Xiaoyi Ma. 2015. From light to rich ere: annotation of entities, relations, and events. In </a><i>Proceedings of the the 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representa- tion</i>, pages 89–98.</p><p class="s5" style="padding-top: 9pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://www.aclweb.org/anthology/W02-2024" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark73">Erik F. Tjong Kim Sang. 2002.  </a><a href="https://www.aclweb.org/anthology/W02-2024" class="a" target="_blank">Introduction to the CoNLL-2002 shared task: Language-independent named entity </a><span style=" color: #00007F;">recognition</span>.  In <i>COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002)</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a name="bookmark74">Denny Vrandecˇic´ and Markus Krötzsch. 2014. Wiki- data: a free collaborative knowledge base. </a><i>Commu- nications of the ACM</i>, 57(10).</p><p class="s5" style="padding-top: 9pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark75">David Wadden, Ulme Wennberg, Yi Luan, and Han- naneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In </a><i>Proceedings of the 2019 Conference on Empiri- cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP2019)</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark76">Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006.  Ace 2005 multilin- gual training corpus. </a><i>Linguistic Data Consortium, Philadelphia</i>, 57.</p><p class="s5" style="padding-top: 9pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a name="bookmark77">Haoyang Wen, Yanru Qu, Heng Ji, Qiang Ning, Jiawei Han, Avi Sil, Hanghang Tong, and Dan Roth. 2021. Event time extraction and propagation via graph at- tention networks. In </a><i>Proc. The 2021 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Tech- nologies (NAACL-HLT2021)</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 23pt;text-indent: -10pt;text-align: justify;"><a href="https://doi.org/10.18653/v1/P18-4009" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank" name="bookmark78">Hang Yang, Yubo Chen, Kang Liu, Yang Xiao, and Jun Zhao. 2018. </a><a href="https://doi.org/10.18653/v1/P18-4009" class="a" target="_blank">DCFEE: A document-level Chi- nese financial event extraction system based on au- tomatically labeled training </a><span style=" color: #00007F;">data</span>.  In <i>Proceedings of ACL 2018, System Demonstrations</i>, pages 50– 55, Melbourne, Australia. Association for Compu- tational Linguistics.</p><p class="s5" style="padding-top: 9pt;padding-left: 24pt;text-indent: -10pt;text-align: justify;"><a name="bookmark79">Adnan Yazici, Murat Koyuncu, Turgay Yilmaz, Saeid Sattari, Mustafa Sert, and Elvan Gulen. 2018. An</a></p><p class="s5" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">intelligent multimedia information system for multi- modal content extraction and querying. <i>Multimedia Tools and Applications</i>, 77(2):2225–2260.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark80">Xiaodong Yu, Wenpeng Yin, and Dan Roth. 2020. Paired representation learning for event and entity coreference. </a><i>arXiv preprint arXiv:2010.12808</i>.</p><p class="s5" style="padding-top: 9pt;padding-left: 28pt;text-indent: -10pt;text-align: justify;"><a name="bookmark81">Boliang Zhang, Ying Lin, Xiaoman Pan, Di Lu, Jonathan May, Kevin Knight, and Heng Ji. 2018. Elisa-edl: A cross-lingual entity extraction, linking and localization system. In </a><i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demon- strations</i>, pages 41–45.</p><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark82">Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot,</a></p><p class="s5" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">A. Sabharwal, and D. Roth. 2020. Temporal rea- soning on implicit events from distant supervision. <i>ArXiv</i>, abs/2010.12753.</p></body></html>
